{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook dependencies\n",
    "!source pdf-env/bin/activate # initiating virtual environment\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# ignore all warning messages\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "\n",
    "import os # for caching purposeses\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# plotly module/library\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "\n",
    "# regular expression import\n",
    "import re\n",
    "\n",
    "# JSON import\n",
    "import json\n",
    "\n",
    "# uni-code library\n",
    "import unicodedata\n",
    "\n",
    "# natural language toolkit library/modules\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the pdf file\n",
    "\n",
    "reader = PdfReader(\"ai_report.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the 'reader' object?\n",
    "\n",
    "type(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the total number of pages in the PDF\n",
    "# 'pages' is a method that will return the total number of pages as a virtual list\n",
    "\n",
    "print(f\"There are {len(reader.pages)} Pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the first page (index 0) \n",
    "# page = reader.pages[0]\n",
    "# # Use extract_text() to get the text of the page\n",
    "# print(page.extract_text())\n",
    "\n",
    "# # go through every page and get the text\n",
    "# for i in range(len(reader.pages)):\n",
    "#   page = reader.pages[i]\n",
    "#   print(page.extract_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the PDF file\n",
    "# with open('ai_report.pdf', 'rb') as file:\n",
    "#     reader = PdfReader(file)\n",
    "\n",
    "#     # create a text file for writing\n",
    "#     with open('ai_report.txt', 'w') as output_file:\n",
    "\n",
    "#         # go through every page and write the text to the file\n",
    "#         for i in range(len(reader.pages)):\n",
    "#             page = reader.pages[i]\n",
    "#             text = page.extract_text()\n",
    "\n",
    "#             # write text in 4,000 character chunks with two line breaks\n",
    "#             chunk_size = 4000\n",
    "#             for j in range(0, len(text), chunk_size):\n",
    "#                 chunk = text[j:j + chunk_size]\n",
    "#                 output_file.write(chunk)\n",
    "#                 output_file.write('\\n\\n')\n",
    "\n",
    "#         print(\"Text has been written to the file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's convert the txt file into a csv file to more easily work with it\n",
    "\n",
    "data = pd.read_csv('ai_report.txt', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now writing the data into a CSV file \n",
    "\n",
    "data.to_csv('ai_report.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read-in the CSV file as a pandas Dataframe\n",
    "\n",
    "df = pd.read_csv('ai_report.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the df shape\n",
    "\n",
    "print(f'initial df shape: {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking what the df looks like\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's rename the column header \n",
    "\n",
    "df = df.rename(columns={'Building a Tech Future': 'text'})\n",
    "df.head() # rename did not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see if there are any special characters in the column header\n",
    "\n",
    "df.columns # here we see that there are spaces in the column header\n",
    "# we'll want to account for this, but simplest way will be to copy and past into previous renaming syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's rename the column header \n",
    "\n",
    "df = df.rename(columns={'Building a Tech Future  ': 'text'})\n",
    "df.head() # great, checks out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the dataframe information\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial word count\n",
    "\n",
    "df_copy = df.copy()\n",
    "df_copy[\"text_count\"] = df_copy[\"text\"].str.split().str.len()\n",
    "df_copy[\"text_count\"].sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ``Data Cleaning``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to clean and normalize the current text data\n",
    "\n",
    "def initial_clean(text: pd.Series):\n",
    "    '''Key text cleaning functions'''\n",
    "\n",
    "    # lowercase the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # return only alphanumeric values in text: everything else, convert to whitespace\n",
    "    text = re.sub(\"[^a-z0-9\\s']\", ' ', text)\n",
    "\n",
    "    # cleans multi-line texts in the data\n",
    "    text = re.sub(r\"[\\r|\\n|\\r\\n]+\", ' ', text)\n",
    "\n",
    "    # removing any word/ele <= 2 letters\n",
    "    text = re.sub(r'\\b[a-z]{,2}\\b', '', text)\n",
    "    \n",
    "    # removing multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # removing beginning and end whitespaces\n",
    "    text = text.strip()\n",
    "\n",
    "    # return the text text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check the head first and then apply the initial clean function\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the initial cleaning function\n",
    "\n",
    "df['text'] = df['text'].apply(initial_clean)\n",
    "df.head() # appears to check out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word count\n",
    "\n",
    "df[\"text\"].str.split().str.len().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a function/method to convert the data type as string for easier string manipulation\n",
    "\n",
    "def clean_data_object(column: str, df: pd.DataFrame):\n",
    "    '''Function that takes in a column name and dataframe and converts the\n",
    "    column into a str type data structure for easier string manipulation'''\n",
    "\n",
    "    # let's first create a new copy of the dataframe\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # update the data type to str\n",
    "    df_copy[column] = df_copy[column].astype(str)\n",
    "\n",
    "    # lastly, return the copy of the initial dataframe with the text column as str type\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's apply the created function and check the df info\n",
    "\n",
    "cleaned_df = clean_data_object('text', df)\n",
    "cleaned_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's remove 'stopper words' for better text interpretability and future modeling\n",
    "# we'll do this by creating a function called remove stopwords\n",
    "\n",
    "def remove_stopwords(text: pd.Series, exclude_words = None):\n",
    "    '''Function that removes stop words in text'''\n",
    "\n",
    "    # words to include due to redudancy and regardless of being stopwords\n",
    "    include_words = [\n",
    "                    'ai',\n",
    "                    'artificial',\n",
    "                    'intelligence',\n",
    "                    'machine',\n",
    "                    'learning',\n",
    "                    'deep',\n",
    "                    'neural',\n",
    "                    'networks',\n",
    "                    'algorithm',\n",
    "                    'model',\n",
    "                    'training',\n",
    "                    'inference',\n",
    "                    'predictive',\n",
    "                    'analytics',\n",
    "                    'automation',\n",
    "                    'autonomous',\n",
    "                    'decision',\n",
    "                    'making',\n",
    "                    'robotics',\n",
    "                    'natural',\n",
    "                    'language',\n",
    "                    'processing',\n",
    "                    'computer',\n",
    "                    'vision',\n",
    "                    'data',\n",
    "                    'science',\n",
    "                    'big',\n",
    "                    'analysis',\n",
    "                    'algorithm',\n",
    "                    'ethics',\n",
    "                    'fairness',\n",
    "                    'interpretability',\n",
    "                    'bias',\n",
    "                    'transparency',\n",
    "                    'privacy',\n",
    "                    'security',\n",
    "                    'reliability',\n",
    "                    'trustworthiness',\n",
    "                    'data',\n",
    "                    'leakage',\n",
    "                    'sensitive',\n",
    "                    'private',\n",
    "                    'confidential',\n",
    "                    'identifiable',\n",
    "                    'personally',\n",
    "                    'person',\n",
    "                    'identity',\n",
    "                    'information',\n",
    "                    'privacy',\n",
    "                    'secure',\n",
    "                    'protection',\n",
    "                    'protected',\n",
    "                    'securely',\n",
    "                    'safely',\n",
    "                    'encryption',\n",
    "                    'de-identification',\n",
    "                    'anonymization',\n",
    "                    'ethical',\n",
    "                    'responsibility',\n",
    "                    'responsible',\n",
    "                    'safety',\n",
    "                    'security',\n",
    "                    'risks'\n",
    "]\n",
    "\n",
    "\n",
    "    # initiating a new list of stopwords\n",
    "    stopword_list = stopwords.words('english')\n",
    "    \n",
    "    # adding the additional stopwords to the list\n",
    "    stopword_list += include_words\n",
    "\n",
    "    # exclude any preidentified words from the stopword list\n",
    "    if exclude_words:\n",
    "\n",
    "        stopword_list = [word for word in stopword_list if word not in exclude_words]\n",
    "\n",
    "    # split the text into individual words -- using the apply function since we're passing in a pd.Series      \n",
    "    words = text.apply(lambda x: x.split())\n",
    "    \n",
    "    # removing words with two or fewer letters\n",
    "    words = words.apply(lambda x: [word for word in x if len(word) > 2])\n",
    "\n",
    "    # filter the text words, and only include words not in stop words list\n",
    "    filtered_words = words.apply(lambda x: [word for word in x if word not in stopword_list])\n",
    "    \n",
    "    # re-join the words into individual text text\n",
    "    filtered_text = filtered_words.apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    # return the text text back: excluding stop words\n",
    "    return filtered_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``These words are often discussed in the context of responsible and ethical AI practices, and considering them as stopwords can help filter out\n",
    "potential data leakage concerns when analyzing text related to these topics.``\n",
    "\n",
    "* ai: The abbreviation for \"artificial intelligence,\" the overarching field of study and development of intelligent machines and systems.\n",
    "* artificial: Pertaining to something that is made or produced by humans, in contrast to something occurring naturally.\n",
    "* intelligence: The capacity to understand, learn, and apply knowledge or skills.\n",
    "* machine: Refers to a device or system that uses mechanical or electronic processes to perform tasks.\n",
    "* learning: The process of acquiring knowledge or skills through study, experience, or teaching.\n",
    "* deep: Relating to deep learning, a subset of machine learning that uses neural networks with multiple layers to learn and represent complex patterns and relationships in data.\n",
    "* neural: Relating to the brain or the computational models inspired by the structure and function of biological neural networks.\n",
    "* networks: Refers to interconnected systems or structures, often used to describe neural networks or network architectures.\n",
    "* algorithm: A set of rules or instructions for solving a problem or performing a specific task, often used in AI and machine learning.\n",
    "* model: A representation or abstraction of a system or concept, often used in AI to refer to mathematical or computational models.\n",
    "* training: The process of teaching a machine learning model by providing it with data and adjusting its parameters to improve its performance.\n",
    "* inference: The process of making predictions or drawing conclusions based on the learned patterns from a trained model.\n",
    "* predictive: Relating to the ability to make predictions or forecasts based on data and models.\n",
    "* analytics: The process of discovering meaningful patterns, insights, or trends in data.\n",
    "* automation: The use of technology to perform tasks or processes with minimal human intervention.\n",
    "* autonomous: Referring to systems or agents that can operate or make decisions independently without human control.\n",
    "* decision: The act of making a choice or selecting an option among different possibilities or courses of action.\n",
    "* making: The process of decision-making or reaching a conclusion based on available information.\n",
    "* robotics: The branch of technology that deals with the design, construction, and operation of robots.\n",
    "* natural: Refers to phenomena or behaviors that occur in the physical world or living organisms, often contrasted with artificial or synthetic.\n",
    "* language: Relating to human language or communication, often used in natural language processing (NLP) and understanding.\n",
    "* processing: The manipulation, analysis, or transformation of data or information by a computer or system.\n",
    "* computer: Refers to devices or machines capable of performing computations or carrying out tasks according to instructions.\n",
    "* vision: Relates to the field of computer vision, which focuses on enabling computers to \"see\" and understand visual data.\n",
    "* science: The systematic study and knowledge of the natural and physical world, often used in the context of data science and AI research.\n",
    "* big: Describes large-scale datasets or systems that require specialized approaches for storage, processing, and analysis.\n",
    "* analysis: The examination, evaluation, or interpretation of data or information to derive insights or draw conclusions.\n",
    "* ethics: The moral principles and guidelines that govern human behavior and decision-making, often discussed in the context of AI ethics.\n",
    "* fairness: The principle of treating individuals or groups impartially and without bias or discrimination, an important consideration in AI.\n",
    "* interpretability: The ability to understand or explain how an AI model or algorithm makes its decisions or predictions.\n",
    "* bias: Refers to the presence of systematic errors or prejudices in data or algorithms, which can lead to unfair or discriminatory outcomes.\n",
    "* transparency: The quality of being open, accountable, and understandable,\n",
    "* data: Refers to information or raw facts used by AI systems.\n",
    "* leakage: Implies the unintentional or unauthorized release of data.\n",
    "* sensitive: Describes data that is confidential or requires special protection.\n",
    "* private: Relates to data that is personal or not meant for public disclosure.\n",
    "* confidential: Denotes information that is intended to be kept secret or private.\n",
    "* identifiable: Pertains to data that can be used to identify individuals.\n",
    "* personally: Relates to information that is specific to an individual person.\n",
    "* person: Refers to an individual or human being.\n",
    "* identity: Relates to the unique characteristics or attributes that define an individual.\n",
    "* information: General term for data or knowledge.\n",
    "* privacy: Concerns the protection of personal information and the right to keep it private.\n",
    "* secure: Refers to the state of being protected from unauthorized access or harm.\n",
    "* protection: Denotes the act of safeguarding or defending against threats.\n",
    "* protected: Describes data or systems that are adequately secured.\n",
    "* securely: Indicates a manner or method that ensures strong security measures.\n",
    "* safely: Denotes an action or process that is free from risk or harm.\n",
    "* encryption: Refers to the conversion of data into a coded form to ensure confidentiality.\n",
    "* de-identification: The process of removing or obscuring identifying information from data.\n",
    "* anonymization: Similar to de-identification, it involves making data anonymous.\n",
    "* ethical: Relates to principles and values governing right and wrong behavior.\n",
    "* responsibility: Denotes being accountable or answerable for one's actions.\n",
    "* safety: Concerns the prevention of harm or danger.\n",
    "* security: Relates to measures and practices for protecting against threats or breaches.\n",
    "* risks: Refers to potential hazards or uncertainties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's filter for any whitespace values in the text column\n",
    "\n",
    "df = cleaned_df[cleaned_df['text'].str.strip() != '']\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check the current sample of text words in the dataframe\n",
    "\n",
    "df[\"text\"].sample(20, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the stopword function\n",
    "\n",
    "df['text'] = remove_stopwords(df['text'])\n",
    "df['text'].sample(20, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the top 25 most used words across the text?\n",
    "# at this stage - we can consider merging all the text since the rows are not unique in any meaningful way\n",
    "\n",
    "combined_text = ' '.join(df['text'])\n",
    "type(combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's sample the string text\n",
    "\n",
    "combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's now tokenize the string into individual words\n",
    "\n",
    "tokens = combined_text.split()\n",
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the list of words\n",
    "\n",
    "tokens[0:20] # great, the list checks out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do one last check of ea. word length \n",
    "# in order to to do this more efficiently, let's convert the list into a pandas Series\n",
    "\n",
    "text_series = pd.Series(tokens)\n",
    "type(text_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the series checks out ok\n",
    "\n",
    "text_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check individual string values for word length <= 2\n",
    "# apply boolean masking to accomplish this task \n",
    "\n",
    "text_series[text_series.apply(lambda x: len(x) <= 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's run a quick value counts on the top 20 most frequent words\n",
    "\n",
    "text_series.value_counts()[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the top 30 bigrams for each program language\n",
    "\n",
    "bigrams = (pd.Series(nltk.ngrams(text_series, 2))\n",
    "            .value_counts()\n",
    "            .head(30))\n",
    "\n",
    "# sorting the bigrams by count in descending order\n",
    "bigrams_sorted = bigrams.sort_values(ascending=True)\n",
    "\n",
    "# plotting the sorted bigrams\n",
    "ax = bigrams_sorted.plot.barh(figsize=(15, 6), color=\"darkcyan\")\n",
    "\n",
    "# plt.title(f'{languages[counter]}: Top 20 Most Frequent Bigrams', fontsize = 15)\n",
    "plt.ylabel('Bigrams')\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Top 30 Most Frequent Bigrams')\n",
    "\n",
    "\n",
    "# cleaning the bigram labels\n",
    "ticks, _ = plt.yticks()\n",
    "labels = [f\"{t[0]} {t[1]}\" for t in bigrams_sorted.index]\n",
    "_ = plt.yticks(ticks, labels)\n",
    "\n",
    "# displaying the total count of each bigram at the end of each bar\n",
    "for i, v in enumerate(bigrams_sorted.values):\n",
    "    ax.text(v + 1, i, str(v), color='black', va='center', fontsize=10)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the top 20 trigrams\n",
    "\n",
    "trigrams = (pd.Series(nltk.ngrams(text_series, 3))\n",
    "            .value_counts()\n",
    "            .head(20))\n",
    "\n",
    "# sorting the trigrams by count in descending order\n",
    "trigrams_sorted = trigrams.sort_values(ascending=True)\n",
    "\n",
    "# plotting the sorted trigrams\n",
    "ax = trigrams_sorted.plot.barh(figsize=(15, 6), color=\"darkcyan\")\n",
    "\n",
    "plt.xlabel(None)\n",
    "plt.ylabel('Trigrams')\n",
    "plt.title('Top 20 Most Frequent Trigrams')\n",
    "\n",
    "# cleaning the trigram labels\n",
    "ticks, _ = plt.yticks()\n",
    "labels = [f\"{t[0]} {t[1]} {t[2]}\" for t in trigrams_sorted.index]\n",
    "_ = plt.yticks(ticks, labels)\n",
    "\n",
    "# displaying the total count of each trigram at the end of each bar with smaller font size\n",
    "for i, v in enumerate(trigrams_sorted.values):\n",
    "    ax.text(v + 1, i, str(v), color='black', va='center', fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the trigram index as a list of single text entries\n",
    "\n",
    "trigram_list = [' '.join(word) for word in trigrams_sorted.index.tolist()]\n",
    "trigram_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### `Initializing ChatGPT`\n",
    "\n",
    "* Key References: \n",
    "    - ``DeepLearning.AI`` [\"Prompt Engineering for Developers\"](https://learn.deeplearning.ai/chatgpt-prompt-eng/lesson/2/guidelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will require the use of the OpenAI and python dotenv libraries\n",
    "# both libraries are pre-installed in the virtual environment\n",
    "\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>`Helper Function`</u>\n",
    "Using OpenAI's `gpt-3.5-turbo` model and the [chat completions endpoint](https://platform.openai.com/docs/guides/chat). \n",
    "\n",
    "This helper function will make it easier to use prompts and look at the generated outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt: str, model = \"gpt-3.5-turbo\"):\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model = model,\n",
    "        messages = messages,\n",
    "        temperature = 0 # this is the degree of randomness of the model's output\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's run a prompt test \n",
    "# initiating the prompt\n",
    "\n",
    "prompt = \"\"\" \n",
    "Generate three (3) unique recipes for creative dishes in JSON format with the following keys:\n",
    "dish_name, originating_country, best_drink_pairing \n",
    "using the following ingredients lemons, salmon, sweet potatoes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the response object\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb59b00fe0f91900a8f6a27185b94ebe85462ef43215a5d3782f78624d925c2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
